# -------------------------------------------------------------------------------------------------
# Imports
# -------------------------------------------------------------------------------------------------

import  time

import  torch
import  numpy               as      np

from    .gp                 import  eval_gp, sample_coefs, fit_gps
from    .latent_space       import  initial_condition_latent, Autoencoder
from    .enums              import  NextStep, Result
from    .physics            import  Physics
from    .latent_dynamics    import  LatentDynamics     
from    .timing             import  Timer
from    .param              import  ParameterSpace



def average_rom(autoencoder     : Autoencoder, 
                physics         : Physics, 
                latent_dynamics : LatentDynamics, 
                gp_dictionary   : dict, 
                param_grid      : np.ndarray):
    """
    

    
    -----------------------------------------------------------------------------------------------
    Arguments
    -----------------------------------------------------------------------------------------------

    physics: A "Physics" object that stores the datasets for each parameter combination. 

    autoencoder: The actual autoencoder object that we use to map the ICs into the latent space.

    param_grid: A 2d numpy.ndarray object of shape (number of parameter combination) x (number of 
    parameters). The i,j element of this array holds the value of the j'th parameter in the i'th 
    combination of parameters.


    -----------------------------------------------------------------------------------------------
    Returns
    -----------------------------------------------------------------------------------------------
    
    A list of numpy ndarray objects whose i'th element holds the latent space initial condition 
    for the i'th set of parameters in the param_grid. That is, if we let U0_i denote the fom IC for 
    the i'th set of parameters, then the i'th element of the returned list is Z0_i = encoder(U0_i).
    """
    if (param_grid.ndim == 1):
        param_grid = param_grid.reshape(1, -1)
    n_test = param_grid.shape[0]

    Z0 = initial_condition_latent(param_grid, physics, autoencoder)

    pred_mean, _ = eval_gp(gp_dictionary, param_grid)

    Zis = np.zeros([n_test, physics.nt, autoencoder.n_z])
    for i in range(n_test):
        Zis[i] = latent_dynamics.simulate(pred_mean[i], Z0[i], physics.t_grid)

    return Zis



def sample_roms(autoencoder : Autoencoder, physics, latent_dynamics, gp_dictionary, param_grid, n_samples):
    '''
        Collect n_samples of ROM trajectories on param_grid.
        gp_dictionary: list of Gaussian process regressors (size of n_test)
        param_grid: numpy 2d array
        n_samples: integer
        assert(len(gp_dictionnary) == param_grid.shape[0])

        output: np.array of size [n_test, n_samples, physics.nt, autoencoder.n_z]
    '''

    if (param_grid.ndim == 1):
        param_grid = param_grid.reshape(1, -1)
    n_test = param_grid.shape[0]

    Z0 = initial_condition_latent(param_grid, physics, autoencoder)

    coef_samples = [sample_coefs(gp_dictionary, param_grid[i], n_samples) for i in range(n_test)]

    Zis = np.zeros([n_test, n_samples, physics.nt, autoencoder.n_z])
    for i, Zi in enumerate(Zis):
        z_ic = Z0[i]
        for j, coef_sample in enumerate(coef_samples[i]):
            Zi[j] = latent_dynamics.simulate(coef_sample, z_ic, physics.t_grid)

    return Zis



def get_fom_max_std(autoencoder : Autoencoder, Zis : np.ndarray) -> int:
    """
    Computes the maximum standard deviation across the parameter space grid and finds the
    corresponding parameter index.

    

    -----------------------------------------------------------------------------------------------
    Arguments
    -----------------------------------------------------------------------------------------------

    autoencoder: The autoencoder. We assume the solved dynamics (whose frames are stored in Zis) 
    take place in the autoencoder's latent space. We use this to decode the solution frames.

    Zis: A 4d numpy array of shape (n_test, n_samples, n_t, n_z) whose i, j, k, l element holds 
    the l'th component of the k'th frame of the solution to the latent dynamics when we use the 
    j'th sample of latent coefficients drawn from the posterior distribution for the i'th testing 
    parameter.

    

    -----------------------------------------------------------------------------------------------
    Returns:
    -----------------------------------------------------------------------------------------------

    An integer. The index of the testing parameter that gives the largest standard deviation. 
    Specifically, for each testing parameter, we compute the STD of each component of the fom 
    solution at each frame generated by samples from the posterior coefficient distribution for 
    that parameter. We compute the maximum of these STDs and pair that number with the parameter. 
    We then return the index of the parameter whose corresponding maximum std (the number we pair
    with it) is greatest.
    """
    # TODO(kevin): currently this evaluate point-wise maximum standard deviation.
    #              is this a proper metric? we might want to consider an average, or L2 norm of std.


    max_std : float = 0.0

    # Cycle through the testing parameters.
    for m, Zi in enumerate(Zis):
        # Zi is a 3d tensor of shape (n_samples, n_t, n_z), where n_samples is the number of 
        # samples of the posterior distribution per parameter, n_t is the number of time steps in
        # the latent dynamics solution, and n_z is the dimension of the latent space. The i,j,k
        # element of Zi is the k'th component of the j'th frame of the solution to the latent 
        # dynamics when the latent dynamics uses the i'th set of sampled parameter values.
        Z_m             : torch.Tensor  = torch.Tensor(Zi)

        # Now decode the frames.
        X_pred_m        : np.ndarray    = autoencoder.decoder(Z_m).detach().numpy()

        # Compute the standard deviation across the sample axis. This gives us an array of shape 
        # (n_t, n_fom) whose i,j element holds the (sample) standard deviation of the j'th component 
        # of the i'th frame of the fom solution. In this case, the sample distribution consists of 
        # the set of j'th components of i'th frames of fom solutions (one for each sample of the 
        # posterior coefficients).
        X_pred_m_std    : np.ndarray    = X_pred_m.std(0)

        # Now compute the maximum standard deviation across frames/fom components.
        max_std_m       : np.float32    = X_pred_m_std.max()

        # If this is bigger than the biggest std we have seen so far, update the maximum.
        if max_std_m > max_std:
            m_index : int   = m
            max_std : float = max_std_m

    # Report the index of the testing parameter that gave the largest maximum std.
    return m_index


# move optimizer parameters to device
def optimizer_to(optim, device):
    for param in optim.state.values():
        # Not sure there are any global tensors in the state dict
        if isinstance(param, torch.Tensor):
            param.data = param.data.to(device)
            if param._grad is not None:
                param._grad.data = param._grad.data.to(device)
        elif isinstance(param, dict):
            for subparam in param.values():
                if isinstance(subparam, torch.Tensor):
                    subparam.data = subparam.data.to(device)
                    if subparam._grad is not None:
                        subparam._grad.data = subparam._grad.data.to(device)

class BayesianGLaSDI:
    X_train = torch.Tensor([])
    X_test = torch.Tensor([])

    def __init__(self, physics, autoencoder : Autoencoder, latent_dynamics, param_space, config):

        '''

        This class runs a full GPLaSDI training. It takes into input the autoencoder defined as a PyTorch object and the
        dictionnary containing all the training parameters.
        The "train" method with run the active learning training loop, compute the reconstruction and SINDy loss, train the GPs,
        and sample a new FOM data point.

        '''

        self.autoencoder = autoencoder
        self.latent_dynamics = latent_dynamics
        self.physics = physics
        self.param_space = param_space
        self.timer = Timer()

        self.n_samples = config['n_samples']
        self.lr = config['lr']
        self.n_iter = config['n_iter']      # number of iterations for one train and greedy sampling
        self.max_iter = config['max_iter']  # maximum iterations for overall training
        self.max_greedy_iter = config['max_greedy_iter'] # maximum iterations for greedy sampling
        self.ld_weight = config['ld_weight']
        self.coef_weight = config['coef_weight']

        self.optimizer = torch.optim.Adam(autoencoder.parameters(), lr = self.lr)
        self.MSE = torch.nn.MSELoss()

        self.path_checkpoint = config['path_checkpoint']
        self.path_results = config['path_results']

        from os.path import dirname
        from pathlib import Path
        Path(dirname(self.path_checkpoint)).mkdir(parents=True, exist_ok=True)
        Path(dirname(self.path_results)).mkdir(parents=True, exist_ok=True)

        device = config['device'] if 'device' in config else 'cpu'
        if (device == 'cuda'):
            assert(torch.cuda.is_available())
            self.device = device
        elif (device == 'mps'):
            assert(torch.backends.mps.is_available())
            self.device = device
        else:
            self.device = 'cpu'

        self.best_loss = np.inf
        self.best_coefs = None
        self.restart_iter = 0

        self.X_train = torch.Tensor([])
        self.X_test = torch.Tensor([])

        return

    def train(self):
        assert(self.X_train.size(0) > 0)
        assert(self.X_train.size(0) == self.param_space.n_train())

        device = self.device
        autoencoder_device = self.autoencoder.to(device)
        X_train_device = self.X_train.to(device)

        from pathlib import Path
        Path(self.path_checkpoint).mkdir(parents=True, exist_ok=True)
        Path(self.path_results).mkdir(parents=True, exist_ok=True)

        ps = self.param_space
        n_train = ps.n_train()
        ld = self.latent_dynamics

        '''
            determine number of iterations.
            Perform n_iter iterations until overall iterations hit max_iter.
        '''
        next_iter = min(self.restart_iter + self.n_iter, self.max_iter)

        for iter in range(self.restart_iter, next_iter):
            self.timer.start("train_step")

            self.optimizer.zero_grad()
            Z = autoencoder_device.encoder(X_train_device)
            X_pred = autoencoder_device.decoder(Z)
            Z = Z.cpu()

            loss_ae = self.MSE(X_train_device, X_pred)
            coefs, loss_ld, loss_coef = ld.calibrate(Z, self.physics.dt, compute_loss=True, numpy=True)

            max_coef = np.abs(coefs).max()

            loss = loss_ae + self.ld_weight * loss_ld / n_train + self.coef_weight * loss_coef / n_train

            loss.backward()
            self.optimizer.step()

            if loss.item() < self.best_loss:
                torch.save(autoencoder_device.cpu().state_dict(), self.path_checkpoint + '/' + 'checkpoint.pt')
                autoencoder_device = self.autoencoder.to(device)
                self.best_coefs = coefs
                self.best_loss = loss.item()

            print("Iter: %05d/%d, Loss: %3.10f, Loss AE: %3.10f, Loss LD: %3.10f, Loss COEF: %3.10f, max|c|: %04.1f, "
                  % (iter + 1, self.max_iter, loss.item(), loss_ae.item(), loss_ld.item(), loss_coef.item(), max_coef),
                  end = '')

            if n_train < 6:
                print('Param: ' + str(np.round(ps.train_space[0, :], 4)), end = '')

                for i in range(1, n_train - 1):
                    print(', ' + str(np.round(ps.train_space[i, :], 4)), end = '')
                print(', ' + str(np.round(ps.train_space[-1, :], 4)))

            else:
                print('Param: ...', end = '')
                for i in range(5):
                    print(', ' + str(np.round(ps.train_space[-6 + i, :], 4)), end = '')
                print(', ' + str(np.round(ps.train_space[-1, :], 4)))

            self.timer.end("train_step")
        
        self.timer.start("finalize")

        self.restart_iter += self.n_iter

        if ((self.best_coefs is not None) and (self.best_coefs.shape[0] == n_train)):
            state_dict = torch.load(self.path_checkpoint + '/' + 'checkpoint.pt')
            self.autoencoder.load_state_dict(state_dict)
        else:
            self.best_coefs = coefs
            torch.save(autoencoder_device.cpu().state_dict(), self.path_checkpoint + '/' + 'checkpoint.pt')

        self.timer.end("finalize")
        self.timer.print()

        return
    
    def get_new_sample_point(self):
        self.timer.start("new_sample")
        assert(self.X_test.size(0) > 0)
        assert(self.X_test.size(0) == self.param_space.n_test())
        assert(self.best_coefs.shape[0] == self.param_space.n_train())
        coefs = self.best_coefs

        print('\n~~~~~~~ Finding New Point ~~~~~~~')
        # TODO(kevin): william, this might be the place for new sampling routine.

        ae = self.autoencoder.cpu()
        ps = self.param_space
        n_test = ps.n_test()
        ae.load_state_dict(torch.load(self.path_checkpoint + '/' + 'checkpoint.pt'))

        Z0 = initial_condition_latent(ps.test_space, self.physics, ae)

        gp_dictionnary = fit_gps(ps.train_space, coefs)

        coef_samples = [sample_coefs(gp_dictionnary, ps.test_space[i], self.n_samples) for i in range(n_test)]

        Zis = np.zeros([n_test, self.n_samples, self.physics.nt, ae.n_z])
        for i, Zi in enumerate(Zis):
            z_ic = Z0[i]
            for j, coef_sample in enumerate(coef_samples[i]):
                Zi[j] = self.latent_dynamics.simulate(coef_sample, z_ic, self.physics.t_grid)

        m_index = get_fom_max_std(ae, Zis)

        new_sample = ps.test_space[m_index, :].reshape(1, -1)
        print('New param: ' + str(np.round(new_sample, 4)) + '\n')

        self.timer.end("new_sample")
        return new_sample
        
    def export(self):
        dict_ = {'X_train': self.X_train, 'X_test': self.X_test, 'lr': self.lr, 'n_iter': self.n_iter,
                 'n_samples' : self.n_samples, 'best_coefs': self.best_coefs, 'max_iter': self.max_iter,
                 'max_iter': self.max_iter, 'ld_weight': self.ld_weight, 'coef_weight': self.coef_weight,
                 'restart_iter': self.restart_iter, 'timer': self.timer.export(), 'optimizer': self.optimizer.state_dict()
                 }
        return dict_
    
    def load(self, dict_):
        self.X_train = dict_['X_train']
        self.X_test = dict_['X_test']
        self.best_coefs = dict_['best_coefs']
        self.restart_iter = dict_['restart_iter']
        self.timer.load(dict_['timer'])
        self.optimizer.load_state_dict(dict_['optimizer'])
        if (self.device != 'cpu'):
            optimizer_to(self.optimizer, self.device)
        return
    


class BayesianGLaSDI:
    X_train = None

    def __init__(self, physics, autoencoder, latent_dynamics, model_parameters):

        '''

        This class runs a full GPLaSDI training. It takes into input the autoencoder defined as a PyTorch object and the
        dictionnary containing all the training parameters.
        The "train" method with run the active learning training loop, compute the reconstruction and SINDy loss, train the GPs,
        and sample a new FOM data point.

        '''

        self.autoencoder = autoencoder
        self.latent_dynamics = latent_dynamics
        self.physics = physics
        self.param_space = physics.param_space
        self.timer = Timer()

        self.n_samples = model_parameters['n_samples']
        self.lr = model_parameters['lr']
        self.n_iter = model_parameters['n_iter']
        self.n_greedy = model_parameters['n_greedy']
        self.max_greedy_iter = model_parameters['max_greedy_iter']
        self.sindy_weight = model_parameters['sindy_weight']
        self.coef_weight = model_parameters['coef_weight']

        self.optimizer = torch.optim.Adam(autoencoder.parameters(), lr = self.lr)
        self.MSE = torch.nn.MSELoss()

        self.path_checkpoint = model_parameters['path_checkpoint']
        self.path_results = model_parameters['path_results']

        from os.path import dirname
        from pathlib import Path
        Path(dirname(self.path_checkpoint)).mkdir(parents=True, exist_ok=True)
        Path(dirname(self.path_results)).mkdir(parents=True, exist_ok=True)

        device = model_parameters['device'] if 'device' in model_parameters else 'cpu'
        if (device == 'cuda'):
            assert(torch.cuda.is_available())
            self.device = device
        elif (device == 'mps'):
            assert(torch.backends.mps.is_available())
            self.device = device
        else:
            self.device = 'cpu'

        self.best_loss = np.Inf
        self.restart_iter = 0

        return

    def train(self):
        assert(self.X_train is not None)
        assert(self.X_train.size(0) == self.param_space.n_train)

        device = self.device
        autoencoder_device = self.autoencoder.to(device)
        X_train_device = self.X_train.to(device)

        from pathlib import Path
        Path(self.path_checkpoint).mkdir(parents=True, exist_ok=True)
        Path(self.path_results).mkdir(parents=True, exist_ok=True)

        ps = self.param_space
        ld = self.latent_dynamics

        for iter in range(self.restart_iter, self.n_iter):
            self.timer.start("train_step")

            self.optimizer.zero_grad()
            Z = autoencoder_device.encoder(X_train_device)
            X_pred = autoencoder_device.decoder(Z)
            Z = Z.cpu()

            loss_ae = self.MSE(X_train_device, X_pred)
            coefs, loss_sindy, loss_coef = ld.calibrate(Z, self.physics.dt, compute_loss=True, numpy=True)

            max_coef = np.abs(coefs).max()

            loss = loss_ae + self.sindy_weight * loss_sindy / ps.n_train + self.coef_weight * loss_coef / ps.n_train

            loss.backward()
            self.optimizer.step()

            if loss.item() < self.best_loss:
                torch.save(autoencoder_device.state_dict(), self.path_checkpoint + '/' + 'checkpoint.pt')
                self.best_coefs = coefs
                self.best_loss = loss.item()

            print("Iter: %05d/%d, Loss: %3.10f, Loss AE: %3.10f, Loss SI: %3.10f, Loss COEF: %3.10f, max|c|: %04.1f, "
                  % (iter + 1, self.n_iter, loss.item(), loss_ae.item(), loss_sindy.item(), loss_coef.item(), max_coef),
                  end = '')

            if ps.n_train < 6:
                print('Param: ' + str(np.round(ps.train_space[0, :], 4)), end = '')

                for i in range(1, ps.n_train - 1):
                    print(', ' + str(np.round(ps.train_space[i, :], 4)), end = '')
                print(', ' + str(np.round(ps.train_space[-1, :], 4)))

            else:
                print('Param: ...', end = '')
                for i in range(5):
                    print(', ' + str(np.round(ps.train_space[-6 + i, :], 4)), end = '')
                print(', ' + str(np.round(ps.train_space[-1, :], 4)))

            self.timer.end("train_step")

            if ((iter > self.restart_iter) and (iter < self.max_greedy_iter) and (iter % self.n_greedy == 0)):

                if (self.best_coefs.shape[0] == ps.n_train):
                    coefs = self.best_coefs

                new_sample = self.get_new_sample_point(coefs)

                # TODO(kevin): implement save/load the new parameter
                ps.appendTrainSpace(new_sample)
                self.restart_iter = iter
                next_step, result = NextStep.RunSample, Result.Success
                print('New param: ' + str(np.round(new_sample, 4)) + '\n')
                # self.timer.end("new_sample")
                return result, next_step
        
        self.timer.start("finalize")

        if (self.best_coefs.shape[0] == ps.n_train):
            coefs = self.best_coefs

        gp_dictionnary = fit_gps(ps.train_space, coefs)

        bglasdi_results = {'autoencoder_param': self.autoencoder.state_dict(), 'final_X_train': self.X_train,
                           'coefs': coefs, 'gp_dictionnary': gp_dictionnary, 'lr': self.lr, 'n_iter': self.n_iter,
                           'n_greedy': self.n_greedy, 'sindy_weight': self.sindy_weight, 'coef_weight': self.coef_weight,
                           'n_samples' : self.n_samples,
                           }
        bglasdi_results['physics'] = self.physics.export()
        bglasdi_results['parameters'] = self.param_space.export()
        # TODO(kevin): restart capability for timer.
        bglasdi_results['timer'] = self.timer.export()
        bglasdi_results['latent_dynamics'] = self.latent_dynamics.export()

        date = time.localtime()
        date_str = "{month:02d}_{day:02d}_{year:04d}_{hour:02d}_{minute:02d}"
        date_str = date_str.format(month = date.tm_mon, day = date.tm_mday, year = date.tm_year, hour = date.tm_hour + 3, minute = date.tm_min)
        np.save(self.path_results + '/' + 'bglasdi_' + date_str + '.npy', bglasdi_results)

        self.timer.end("finalize")
        self.timer.print()

        next_step, result = None, Result.Complete
        return result, next_step
    
    def get_new_sample_point(self, coefs):
        self.timer.start("new_sample")

        print('\n~~~~~~~ Finding New Point ~~~~~~~')
        # TODO(kevin): william, this might be the place for new sampling routine.

        ae = self.autoencoder.cpu()
        ps = self.param_space
        ae.load_state_dict(torch.load(self.path_checkpoint + '/' + 'checkpoint.pt'))

        Z0 = initial_condition_latent(ps.test_space, self.physics, ae)

        gp_dictionnary = fit_gps(ps.train_space, coefs)

        coef_samples = [sample_coefs(gp_dictionnary, ps.test_space[i], self.n_samples) for i in range(ps.n_test)]

        Zis = np.zeros([ps.n_test, self.n_samples, self.physics.nt, ae.n_z])
        for i, Zi in enumerate(Zis):
            z_ic = Z0[i]
            for j, coef_sample in enumerate(coef_samples[i]):
                Zi[j] = self.latent_dynamics.simulate(coef_sample, z_ic, self.physics.t_grid)

        m_index = get_fom_max_std(ae, Zis)

        self.timer.end("new_sample")
        return ps.test_space[m_index, :]
    
    def sample_fom(self):

        new_foms = self.param_space.n_train - self.X_train.size(0)
        assert(new_foms > 0)
        new_params = self.param_space.train_space[-new_foms:, :]

        if not self.physics.offline:
            new_X = self.physics.generate_solutions(new_params)

            self.X_train = torch.cat([self.X_train, new_X], dim = 0)
        else:
            # TODO(kevin): interface for offline FOM simulation
            raise RuntimeError("Offline FOM simulation is not supported yet!")